\documentclass[11pt, a4paper]{article} %tamaño mínimo de letra 11pto.

\usepackage{graphicx} 
\usepackage{subcaption}
\usepackage[spanish]{babel} %Español 
\usepackage[utf8]{inputenc} %Para poder poner tildes
\usepackage{vmargin} %Para modificar los márgenes
\setmargins{2.5cm}{1.5cm}{16.5cm}{23.42cm}{10pt}{1cm}{0pt}{2cm}
%margen izquierdo, superior, anchura del texto, altura del texto, altura de los encabezados, espacio entre el texto y los encabezados, altura del pie de página, espacio entre el texto y el pie de página
\usepackage{hyperref}
\usepackage{listings} 
\usepackage{xcolor}
% Define custom colors to match your screenshot
\definecolor{kwcolor}{RGB}{197,134,192}    % pinkish for keywords
\definecolor{comcolor}{RGB}{106,153,85}    % green for comments
\definecolor{strcolor}{RGB}{206,145,120}   % salmon for strings
\definecolor{typecolor}{RGB}{86,156,214}   % light blue for types/modules
\definecolor{funccolor}{RGB}{220,220,170}  % yellowish for function calls
\definecolor{varcolor}{RGB}{156,220,254}   % cyan for imported variables
\definecolor{numcolor}{RGB}{181,206,168}   % soft green for numbers

% Configure listings for Python
\lstset{
  language=Python,
  frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{kwcolor}\bfseries,
  commentstyle=\color{comcolor}\itshape,
  stringstyle=\color{strcolor},
  numberstyle=\tiny\color{gray},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4,
  emphstyle=\color{typecolor},
  alsoletter={.}, % allows highlighting dot-access
  literate=%
    *{=}{{=}}1
     {+}{{+}}1
     {-}{{-}}1
     {*}{{*}}1
     {>}{{>}}1
     {<}{{<}}1
}
\usepackage{csquotes}% Recommended
\usepackage[minnames=1,maxnames=2,maxbibnames = 3, style=authoryear, bibstyle = authoryear, backend=bibtex, giveninits=true, block = none, isbn = false, url = false, doi = false, eprint = false]{biblatex}
\usepackage{float}
\addbibresource{refs.bib} %Imports bibliography file

\begin{document}
%%%%%%Portada%%%%%%%
\begin{titlepage}
\centering
{ \bfseries \Large UNIVERSIDAD COMPLUTENSE DE MADRID}
\vspace{0.5cm}

{\bfseries  \Large FACULTAD DE CIENCIAS FÍSICAS} 
\vspace{1cm}

{\large DEPARTAMENTO DE FÍSICA DE LA TIERRA Y ASTROFÍSICA}
\vspace{0.8cm}

%%%%Logo Complutense%%%%%
{\includegraphics[width=0.35\textwidth]{logo_UCM.png}} %Para ajustar la portada a una sola página se puede reducir el tamaño del logo
\vspace{0.8cm}

{\bfseries \Large TRABAJO DE FIN DE GRADO}
\vspace{2cm}

{\Large Código de TFG:  [C\'odigo TFG] } \vspace{5mm}

{\Large [Relaciones estructurales de galaxias remotas a partir de los catálogos CANDELS]}\vspace{5mm}

{\Large [Structural relations of remote galaxies from the CANDELS catalogues]}\vspace{5mm}

{\Large Supervisor/es: [Nombre del/os supervisores]}\vspace{20mm} 

{\bfseries \LARGE Jesús Gallego Maestro}\vspace{5mm} 

{\large Grado en Física}\vspace{5mm} 

{\large Curso acad\'emico 20[24-25]}\vspace{5mm} 

{\large Convocatoria XXXX}\vspace{5mm} 

\end{titlepage}
\newpage

%{\bfseries \large [Título extendido del TFG (si procede)] }\vspace{10mm} 

{\bfseries \large Resumen:} \vspace{5mm}

La busqueda de patrones en los datos de experimentos cientificos es una de las principales
fuentes de informacion en el campo de la astrofísica. El universo a grandes escalas está 
compuesto por procesos muy complejos, por lo que la
observación de estos patrones nos permite entender mejor el funcionamiento de estos procesos.

Durante muchos años, se ha observado el firmamento con nuevas y mejores tecnologías con el 
fin de tener una mayor cantidad de datos de los que obtener información. El catálogo CANDELS
contiene una gran cantidad de datos sobre galaxias,  a varios redshifts diferentes, que 
contiene una gran cantidad de datos sobre propiedades físicas acercas de estas. 

El objetivo es usar este catálogo para analizar los datos con un algoritmo de regresión simbólica 
para obtener relaciones ya conocidas, y ver que otras relaciones se pueden obtener a partir de 
estos datos.

[Rsumen de lo encontrado]
[Conclusiones]
[Prespectiva]
\vspace{10mm}

{\bfseries \large Abstract: } \vspace{5mm} 

The search of patterns in scientific experiments data is one of the main sources of 
information in astrophysics. The vast universe is characterized of complex, that makes the
observation of these patterns can help us to understand better the working of these processes.

For many years, the cosmic dust has been observed with new and better technologies to obtain a 
higher amount of data. The CANDELS catalogue contains a large amount of data about galaxies, 
at different redshifts, that contains a large amount of data about physical properties of these objects.

The goal is to use this catalogue to analyze the data with a symbolic regression algorithm to obtain 
known relations, and see that other relations can be obtained from these data.

[Abstract of what has been found]
[Conclusions]
[Outlook]

\vspace{1cm}

%%Comentar estas notas para que no salgan en la memoria
%{\Large\textbf{Nota: el título extendido (si procede), el resumen y el abstract deben estar en una misma página y su extensión no debe superar una página. Tamaño mínimo 11pto.}}
%\vspace{1cm}

%{\Large\textbf{Extensión máxima 20 páginas sin contar portada ni resumen (sí se incluye índice, introducción, conclusiones y bibliografía}}
\newpage

%%Inicio:
\tableofcontents


\section{Introducción}

El objetivo principal de la física es tener la capcaidad de entender, y por consecuente, poder explicar los procesos que ocurren en 
nuestro universo. Desde hace ya varios siglos la observación ha sido una de las herramientas más útiles para poder tener indicios del comportamiento de estos procesos,
como la relación $r\propto T^3$ descubierta por Kepler, o la predicción de nuevas partículas basandose en las propiedades y simetrias de un 
determinado conjutno de partículas. \\

Es por ello que las relaciones que podemos obtener a partir de los datos experimentales propician el descubrimiento de nueva física, especialmente en la astrofísica.
Las relaciones entre los diferentes parámetros de las galaxias, como el radio, el tamaño, la masa, etc, nos permiten clasificarlas según como estas se relacionan entre si. 
Este tipo de clasificaciones, meramente observacionales, son muy habituales en la astrofísica extragaláctica entre ellos destacan algunos puramente observacionales como puede ser el diagrama de Hubble, 
y otros más analíticos como puede ser el índice de Sersix. Ambos tienen un único objetivo, clasificar las galaxias con el fin de poder estudiar y comparar los sistemas con otros que sufren procesos similares debido a que pertenecen al mismo grupo.\\

Este trabajo se centra en estudiar un grupo conrecto de galaxias, aquellas galaxias que pertenecen al \textit{Cosmic Noon} o 
amanezer cósmico (Ver figura \ref{fig:madau}). En este periodo del universo (con valores de redshift entre $1.5$ y $2.5$) se formaron la mayoría de galaxias que 
observamos en el universo actual, es por eso que su estudio puede revelar una gran cantidad de información no solo acerca de los orígenes
de estas estructuras, si no también de su evolución hasta llegar a las galaxias del universo cercano. \\

%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=0.5]{Madau-plot-of-the-star-formation-rate-7.png}
%    \caption{Diagrama de Madau en donde se aprecia el máximo de formación de galaxias. \autocite{Madau}}
%    \label{fig:madau}
%\end{figure}

Dada la morfología de este grupo de galaxias, la cual es muy diferente de las actuales, es de esperar que sus relaciones estructurales 
no tengan porque ser las mismas, o que en caso de que si sean la misma no se relacionen de la misma forma. 
\section{La exploración CANDELS}

La exploración (\textit{Cosmic Assembly Near-IR Deep Extragalactic Legacy Survey (CANDELS)})\autocite{Grogin,Koekemoer}



Los objetivos de la exploración pueden verse en la tabla 2 en \autocite{Grogin}

\subsection{El catálogo EGS}

La exploración CANDELS brinda una gran fuente de información de multitud de fuentes con las que trabajar, sin embargo, este
trabajo se va a centrar el estudio en el catálogo \textit{Extended Growth Strip (EGS)} el cual se centra en 

\autocite{Stefanon,Kodra_2023}
\section{Regresión simbólica}

La regresión simbólica (SR) es un tipo de ajuste a los datos en el que no se parte de un 
modelo concreto, como puede ser la regresión lineal o ajustar a una exponencial, sino que 
se parte de una serie de operadores o funciones, como puede ser el uso de funciones trigonométricas 
o usar funciones con exponentes, que busca el mejor modelo como combinación de esos operadores.  


Los algoritmos de SR despuntaron en la decada de los 80 como respuesta a la difícil tarea de encontrar ecuaciones que 
ajustaran a unos datos experimentales que crecian tanto en volumen como en dimensión. Es por eso que muchos métodos diferentes
se han presentado desde entonces con diferentes puntos de vista. La idea detrás de estos algoritmos es facilitar la búsqueda
de expresiones analíticas con las que luego poder hacer otros cálculos (ecuaciones de continuidad, condiciones de contorno, etc)
y derivar conclusiones de los posibles mecanismo causantes del comportamiento de los datos. 


En este caso concreto se ha usado la librería \href{https://github.com/MilesCranmer/PySR}{PySR} la cual
ha sido elegida por su impletenciación en Python, por los ''benchmarks''  o resultados que 
se describen en \autocite{cranmerInterpretableMachineLearning2023}. Además, al tratarse de una libreria reciente y de codigo abierto,
PySR se encuentra en constante crecimiento haciendo que la comunidad científia en su conjunto pueda apoyar su desarrollo y mejora.


A parte, aunque no se entrará en gran detalle esta libreria tiene
una gran cantidad de funcionalidades que la hacen muy polivalente para una gran cantidad de usos, además de una integración con varias
librerias que permiten un futuro tratamiento de las expresiones obtenidas. A continuación se explica el mecanismo de funcionamiento
de este algortimo en concreto.
\subsection{Fundamento}

PySR se basa en un algoritmo de SR general al que sus autores le han realizado algunas modificaciones con el fin de facilitar
su versatilidad en cuanto a la aplicación a diversos campos. 

A continuación se describe el algoritmo de convergencia, en lineas generales, para la obtención de expresiones matemáticas;
junto con las modificaciones que presenta PySR. El algoritmo
consiste en el siguiente conjunto de pasos:
\begin{itemize}
    \item Bucle interno
    
    El bucle interno consiste en los siguientes pasos:
    \begin{enumerate}
        \item Supongamos una expresión matemática con un conjunto $N$ de combinaciones de operadores que queremos evaluar y vamos
        a comparar cada conjunto de operadores entre si, esto se denomina \textbf{torneo}. Cada torneo consiste en enfrentar
        $n_{s}$ subconjuntos de operadores (tipicamente $n_{s}=2$) y enfrentamos a cada operador $O_{i}$ entre si.
        \item Asignamos una probabilidad $p$ al operador que mejor ajuste tras compararlos todos. Si $p$ es muy bajo, quitamos 
        ese operador del subconjunto y repetimos este paso.
        \item Nos quedamos con aquel operador con mayor probabilidad y lo metemos en otro de los subconjuntos de operadores $O$ 
        para repetir el proceso hasta que converga del cual obtendremos el conjunto de operadores que define el mejor ajuste.
        
    \end{enumerate}
    \item Bucle externo
    
    El bucle externo consiste en realizar el proceso para diferentes conjuntos $N$ (\textbf{islas}) y así tener la posibilidad de migrar
    de una isla a otra y aumentar las combinaciones posibles. 
\end{itemize}



\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Inner_loop.png}
        \caption{Bucle interno del algoritmo }
        \label{fig:sub11}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Islas.png}
        \caption{Bucle externo del algoritmo}
        \label{fig:sub22}
    \end{subfigure}
    \caption{Diagrama de los bucles utilizados en PYSR. \autocite{cranmerInterpretableMachineLearning2023}}
    \label{fig:imagenes_lado_a_ladoo}
\end{figure}

A continuación vamos a ver como se ha implementado el modelo de SR para obtener resultados.


\begin{lstlisting}[language=Python, caption=Implementación del modelo de regresión simbólica, basicstyle=\tiny,label=lst:foo]
    def symbolic_regression(df, input_cols, target_col, threshold=0.2):
        """
        Esta función procesa los datos, ajusta el modelo de regresión simbólica (SR) y cuantifica su ajuste.
    
        _________________________________ENTRADAS________________________________
        df: DataFrame con los datos originales.
        input_cols: Lista con 1 o 2 columnas del DataFrame usadas como variables independientes.
                    - Si es 1, se asume una relación 2D.
                    - Si son 2, se asume una relación 3D.
        target_col: Nombre de la columna objetivo (variable dependiente).
        threshold: Umbral para la varianza explicada por la tercera componente (en caso 3D).
                   Por defecto es 0.2, es decir, el modelo es aceptable si el 80% de los datos
                   están contenidos en el plano de entrada.
    
        _________________________________SALIDAS________________________________
        - best_expr: Mejor expresión simbólica encontrada (en formato SymPy) si cumple el umbral.
        - métrica: Varianza explicada o RMSE según el caso.
        - datos: Datos combinados usados para visualización o evaluación.
        """
    
        # Extraer variables independientes y dependientes
        X = df[input_cols].values
        y = df[target_col].values
    
        # Dividir los datos en entrenamiento y prueba
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=12
        )
    
        # Ajustar el modelo de regresión simbólica con los datos de entrenamiento
        model = PySRRegressor(
            binary_operators=["+", "-", "*", "/", "^"],
            unary_operators=["exp", "log", "sqrt"],
            model_selection="best",
            verbosity=0,
            constraints={'^': (-2, 2)}
        )
        model.fit(X_train, y_train)
    
        # Predecir con el modelo ajustado
        y_pred = model.predict(X_test)
    
        # Normalizar los datos para evaluación (evita sesgos por escalas)
        scaler_X = StandardScaler()
        X_test_scaled = scaler_X.fit_transform(X_test)
    
        scaler_y = StandardScaler()
        y_test_scaled = scaler_y.fit_transform(y_test.reshape(-1, 1)).ravel()
        y_pred_scaled = scaler_y.transform(y_pred.reshape(-1, 1)).ravel()
    
        # Juntar las predicciones con los datos de entrada
        PCA_data = np.hstack((X_test, y_pred.reshape(-1, 1)))
    
        if len(input_cols) == 2:
            # Caso tridimensional: evaluar el plano de entrada y su capacidad explicativa
    
            # Combinar entradas normalizadas y predicción para PCA
            data_combined = np.hstack((X_test_scaled, y_pred_scaled.reshape(-1, 1)))
    
            # Aplicar PCA a 3 componentes
            pca = PCA(n_components=3)
            pca.fit(data_combined)
            explained_variance = pca.explained_variance_ratio_
    
            # Varianza explicada por la tercera componente (z)
            third_component_variance = explained_variance[2]
    
            print(f"Varianza explicada por la tercera componente: {third_component_variance:.4f}")
    
            # Si la varianza es menor al umbral, aceptamos el modelo
            if third_component_variance <= threshold:
                best_expr = model.sympy()
                return best_expr, third_component_variance, PCA_data
            else:
                return None, third_component_variance, PCA_data
    
        else:
            # Caso bidimensional: usar error cuadrático medio como métrica
            rmse = root_mean_squared_error(y_test_scaled, y_pred_scaled)
            print(f"RMSE: {rmse:.4f}")
    
            # Retornar la mejor expresión encontrada junto con la métrica
            best_expr = model.sympy()
            return best_expr, rmse, y_pred, y_test
    \end{lstlisting}

De esta forma podemos obtener las expresiones, evaluarlas y cuantificar su calidad.

\section{Metodología}

Para proceder al análisis de los datos lo primero es centrarse en posbles relaciones ya conocidas de forma que podamos reducir el número de posibles relaciones, como ya se ha comentado 
en la sección (EGS), no todas las relaciones son interesante, ya sea por el tipo de dato que tenemos o porque no haya una motivación física detrás con la que se pueda argumentar la existencia de dicha relación
\footnote{Aunque lo ideal fuera no tener ningún sesgo, dado la dimensionalidad del conjunto de datos lo óptimo es filtrar por relaciones ya conocidas y hacer variaciones de las mismas para explorar otras nuevas}.

Una forma sencilla, visual y efectiva de iniciar la busqueda de relaciones es mediante el uso de \textit{Pairplots}. Este tipo de gráficos nos permiten tener de un primer vistazo una imagen del comportamiento de los datos. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{custom_pairplots/SFR/Fontana_SFR_pairplot.png}
    \caption{Ejemplo de Pairplot. En este caso se muestra un Pairplot para los datos de tasa de formación estelar de Fontana}
    \label{fig:pair_ex}
\end{figure}

Como se puede ver en la figura \ref{fig:pair_ex} con este tipo de gráficos podemos obtener una gran cantidad de información para entenderla naturaleza de los datos:

\begin{itemize}
    \item \textbf{Triángulo inferior}: En el tríangulo inferior podemos observar un diagrama KDE \textit{Kernel Density Estimator} el cual nos 
    permite ver la distribución de los datos, es especialmente útil para poder observar distintos grupos de datos y su distribución.
    \item \textbf{Diagonal}: En la diagonal es posible observar la distribución de los datos. Como es de esperar los datos no tienen porque seguir una distribución normal o gaussiana, esto se debe al \textit{Sesgo del superviviente}, dado que estamos estudiando
     una región muy concreta del espacio, no solo por el campo de la exploración si no también por centrarnos en el \textit{Cosmic Noon}, es muy probable que no tengamos galaxias muy diferentes entre si
    \item \textbf{Triángulo superior}: El triangulo superior consiste un simple gráfico de puntos donde podemos ver mejor cual es el comportamiento de los datos.
    
\end{itemize}

Haciendo uso de estos gráficos podemos obtener de forma visual información sobre el comportamiento de los datos, no solo en cuanto a la dependencia de una variable con respecto a otra; si no 
también como se distribuyen o si froman agrupaciones. 

Dadas las dimensiones de estos gráficos y el número límitado de páginas de este trabajo a continuación se muestran las conclusiones que se ha obtenido con ellos\footnote{Los gráficos pueden verse en este \href{https://github.com/PhyAMR/TFG/tree/main/custom_pairplots}{enlace}}:


\begin{itemize}
    \item Los diferentes modelos de ajuste a los datos (denotados con tau, invtau, lin, cons, etc) 
    suelen obtener datos similares, en los pairplots se ve que (en muchos casos) forman una recta cuando se gráfican entre sí. Esto nos permite poder comparar diferentes modelos entre si y cambiar el modelo sencillamente usando la recta de ajuste.
    \item En el caso de las tasas de formación estelar, esto no se cumple, y son más dependientes del modelo.
    \item El primer punto solo es aplicable cuando nos comparamos modelos de un mismo autor, ya que aunque siguen formando una recta la dispersión es mucho mayor y no es un procedimiento preciso en este caso.
    \item Al igual ocurre si la medida tiene en cuenta la emisión nebular o no, en este caso la dispersión es mucho mayor y no se puede comparar entre si.
    \item A continuación una lista de posibles relaciones que quedan excluidas una vez analizados los gráficos:
    \begin{enumerate}
        \item 
    \end{enumerate}
\end{itemize}



\subsection{Previo y limpieza de los datos}

Una vez acotadas las posibles relaciones que esperamos encontrar es necesario profundizar en la naturaleza de los datos con 
el fin de detectar posibles errores en la medida o valores atípicos que, aunque válidos, se alejen del comportamiento de la mayoría de las medidas. 
Este descarte de los valores atípicos se raliza con el fin de asegurar que los datos puedan ajustar bien a un modelo, en este caso una SR, y poder así analizar 
correctamente las predicciones del modelo. 

La mayor fuente de valores atípicos son medidas de mala calidad, que generalmente se indicaban con el número $-99.0$. Estos valores han de ser eliminados por la simple razón de carecer de sentido físico, en ninguno de los parámetros estudiados tiene sentido que puedan tomar ese valor (ej, masas, edades, magnitudes, ...). 

Otra fuente de valores atípicos viene de la mano de los núcleos activos. Aunque en los datos se señaliza que galaxias podrían contener un AGN (Active Galactic Nuclei) el número de estos es mucho menor que el tamaño de la muestra, por lo que estudiarlos
de la misma forma siendo tan poco representativos y teniendo un comportamiento tan diferente al de una galaxía que no lo tienen (o cuya imagen no tiene la resolución para identificarlo) carece de sentido si el objetivo es ajustar a un modelo. 

\subsection{Análisis de relaciones estructurales 2D}

El análisis de la relaciones entre 2 parámetros es un caso, que aunque simple, muy poderoso. Estas relaciones han sido muy estudiadas para diferentes parámetros 
como pueden ser la relación Masa-Luminosidad o los diagramas H-R que relacionan propiedades observacionales entre si, todo ello con el fin de encontrar relaciones, que aplicadas a muestras no estudiadas, nos permitan obtener información. 

En el caso más simple de todos, una relación lineal, podemos ver que tan buena es esa relación mediante el \textit{ Coeficiente de Pearson} $R^2$; pero 
estas relaciones no siempre tienen que ser lineales, es posible que sean cuadráticas, logarítmicas, exponenciales, etc. En esos caso, cuando la relación no es puramente lineal, el $R^2$ puede ser bajo
pero la relación sigue estando ahí, es por eso que para estudiar estas relaciones sin sesgar por aquellas relaciones que sean lineales se ha hecho uso de lo que se denomina \textit{Distancia de Correlación} \autocite{dcor}.

La distancia por correlación es una medida de la dependencia entre dos variables aleatorias, en la que se mide la distancia entre los valores de las variables (distancia entre puntos en un diagrama de dsipersión), por lo que no se ve tan afectado pot la 
linealidad de los datos, es decir, captura la dependencia entre las variables sin importar si esta es lineal o no.

Para estudiar esta relación se han usado los gráficos de dispersión de la figura \ref{fig:MSFR} en donde se han representado los datos en diferentes formas funcionales típicas en astrofísica.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{MSFR/2d_68e6e2.png}
    \caption{Relación entre la masa y la tasa de formación estelar. Pueden verse más gráficos \href{https://github.com/PhyAMR/TFG/tree/main/MSFR}{aquí}}
    \label{fig:MSFR}
\end{figure}

Para obtener las siguientes relaciones finales se ha filtrado el gráfico que presenta el menor \textit{Root Mean Square Error (RMSE)} (Ver \ref{lst:foo}). El RMSE es una medida de la diferencia entre los valores predichos por un modelo y los valores observados. Se define como:
\begin{equation}
    RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
\end{equation}
donde $y_i$ son los valores observados, $\hat{y}_i$ son los valores predichos por el modelo y $n$ es el número total de observaciones. Un RMSE bajo indica que el modelo se ajusta bien a los datos, mientras que un RMSE alto sugiere que el modelo no se ajusta bien, por lo que nos quedamos con el modelo que tiene el menor RMSE.

\subsection{Análisis de relaciones estructurales 3D}

En el caso de las relaciones entre 3 parámetros, el análisis es más complicado, ya que no solo se tiene que tener en cuenta la relación entre los tres parámetros, si no también la forma en la que estos se distribuyen; especialmente para el caso en el que buscamos un \textit{Plano fundamental}. 
Es por eso que es necesario cuantificar cuando aparece un plano al representar los 3 parámetros, para ello se ha empleado el \textit{Explained Variance Ratio (EVR)}. El EVR se define como:
\begin{equation}
    EVR_i=\frac{\lambda_i}{\sum_j^n\lambda_j}
\end{equation}
En donde $\lambda_i$ es el autovalor de la componente $i\in\{x,y,z\}$ y $n$ es el número de componentes principales, en este caso $n=3$.
El EVR es una medida de la varianza explicada por cada componente principal en un análisis de componentes principales (PCA). 
En este caso, el EVR se utiliza para evaluar la cantidad de varianza que se explica por el plano en comparación con la varianza total de los datos. Un EVR alto indica que el plano captura una gran parte de la variabilidad de los datos, lo que sugiere que los datos están bien representados por el plano.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{3d_BooMeLee_SFR_14a_deltau_solMass_yr_z_best.png}
    \caption{Relación entre la masa y la tasa de formación estelar. Pueden verse más gráficos \href{https://github.com/PhyAMR/TFG/tree/main/MSFR}{aquí}}
    \label{fig:3dexample}
\end{figure}

Para obtener la expresión final se filtrado por el EVR de cada una de las gráficas del grid y seleccionado la menor de todas (Ver \ref{lst:foo}).



\section{Resultados}

\subsection{Relaciones estructurales en 2D}
\subsubsection{Relación Masa-Tasa de formación estelar}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{MSFR/2d_68e6e2.png}
    \caption{Relación entre la masa y la tasa de formación estelar. Pueden verse más gráficos \href{https://github.com/PhyAMR/TFG/tree/main/MSFR}{aquí}}
    \label{fig:MSFR}
\end{figure}
\subsubsection{Relación Masa-Luminosidad}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{MSFR/2d_68e6e2.png}
    \caption{Relación entre la masa y la luminosidad. Pueden verse más gráficos \href{URL}{aquí}}
    \label{fig:LM}
\end{figure}
\subsubsection{Relación Masa-Magnitud}
\subsubsection{Relación Tasa de formación estelar-Magnitud}
\subsubsection{Relación }
\subsection{Análisis de relaciones estructurales 3D}
\subsubsection{Relación Masa-Redshift-Tasa de formación estelar}
\section{Conclusiones}

Como se puede observar en los pairplots, la variables de metalicidad (met), extinción ($A_{v}$) y exceso de color ($E(B-V)$) 
formán agrupaciones, por lo que lo mejor sería determinar los grupos de galaxias que se definen con estas variables y estudiar cada grupo por separado.

%\bibliographystyle{plainnat} % We choose the "plain" reference style
%\bibliography{refs} % Entries are in the refs.bib file

\printbibliography
%\appendix

%\section{Otras curiosidades destacables}

\end{document}
